# -*- coding: utf-8 -*-
"""submit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vFo_x1haIdb9ac2bALaOOzeggwUCoy_y
"""

import numpy as np
import sklearn
import pandas as pd
import time
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LinearRegression
from scipy.optimize import minimize
from sklearn.svm import LinearSVC
from scipy.optimize import lsq_linear
from sklearn.metrics import accuracy_score
from scipy.linalg import khatri_rao

# Global model (used in evaluation script)
model = None

train_df = pd.read_csv('public_trn.txt', sep=' ', header=None)
test_df = pd.read_csv('public_tst.txt', sep=' ', header=None)

X_train = train_df.iloc[:, :-1].values  # First 8 columns: challenge
y_train = train_df.iloc[:, -1].values   # Last column: response

X_test = test_df.iloc[:, :-1].values
y_test = test_df.iloc[:, -1].values

print(X_train)
print(X_train.shape)
print(X_test)
print(X_test.shape)

print(y_train)
print(y_train.shape)
print(y_test)
print(y_test.shape)

################################
# Non Editable Region Starting #
################################
def my_fit(X_train, y_train):
################################
#  Non Editable Region Ending  #
################################
    global model
    # Apply feature map
    X_feat = my_map(X_train)

    start_time = time.time()
    # Train a linear classifier (Logistic Regression)
    clf = LogisticRegression(C=140, penalty='l2', solver='lbfgs', max_iter=1200)
    clf.fit(X_feat, y_train)
    end_time = time.time()

    # Save model (coefficients and intercept)
    model = (clf.coef_.flatten(), clf.intercept_[0])  # A tuple
    print(f"Training Time: {end_time - start_time:.4f} seconds")
    return model[0], model[1]


"""
################################
# Non Editable Region Starting #
################################
def my_fit(X_train, y_train):
################################
#  Non Editable Region Ending  #
################################
    global model
    # Apply feature map
    X_feat = my_map(X_train)

    start_time = time.time()
    # Train a linear classifier (LinearSVC)
    clf = LinearSVC(C=100, penalty='l2', loss='squared_hinge', tol=1e-4, max_iter=1000, dual=False)
    clf.fit(X_feat, y_train)
    end_time = time.time()

    # Save model (coefficients and intercept)
    model = (clf.coef_.flatten(), clf.intercept_[0])   # A tuple
    print(f"Training Time: {end_time - start_time:.4f} seconds")
    return model[0], model[1]"
"""

################################
# Non Editable Region Starting #
################################
def my_map( X ):
################################
#  Non Editable Region Ending  #
################################
    """
    Takes the test challenges and applies feature mapping to transform them
    into higher-dimensional feature vectors.
    Here, we use the Khatri-Rao product to create feature mappings.
    """
    # Here, X is a 2D array (n_samples x n_features), where each sample has 8 challenge bits
    print(X)
    print(X.T)
    # Apply the Khatri-Rao product: We will treat each challenge as two separate vectors and apply KR product
    X_transformed = khatri_rao(X.T, X.T)  # Apply Khatri-Rao product on each column (after transposing)
    print(X_transformed)
    print(X_transformed.T)
    # Return the transformed features (may want to transpose back for compatibility)
    return X_transformed.T

# Training the model

weights, bias = my_fit(X_train, y_train)
X_feat = my_map(X_train)                  # Shape of X_feat (6400, 64)
print(model)

X_test_feat = my_map(X_test)
W, b = model
logits = X_test_feat @ W + b
y_pred = (logits > 0).astype(int)
print(y_pred)
print(y_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {accuracy:.4f}")         # Accuracy is not in percentage format

################################
# Non Editable Region Starting #
################################
def my_decode(S):
################################
#  Non Editable Region Ending  #
################################

    def index(var, i):
        offset = {'p': 0, 'q': 64, 'r': 128, 's': 192}
        return offset[var] + i

    def build_matrix_A():
        A = np.zeros((65, 256))
        for i in range(64):             # From i=0 to i=63
            if i == 0:
                A[0, index('p', 0)] = 0.5
                A[0, index('q', 0)] = -0.5
                A[0, index('r', 0)] = 0.5
                A[0, index('s', 0)] = -0.5
            else:
                A[i, index('p', i)] = 0.5
                A[i, index('q', i)] = -0.5
                A[i, index('r', i)] = 0.5
                A[i, index('s', i)] = -0.5
                A[i, index('p', i - 1)] = 0.5
                A[i, index('q', i - 1)] = -0.5
                A[i, index('r', i - 1)] = -0.5
                A[i, index('s', i - 1)] = 0.5

        # For bias term b
        A[64, index('p', 63)] = 0.5
        A[64, index('q', 63)] = -0.5
        A[64, index('r', 63)] = -0.5
        A[64, index('s', 63)] = 0.5
        return A



    A = build_matrix_A()
    x0 = np.ones(256) * 1e-6  # Small positive initial guess
    res = minimize(lambda x: np.linalg.norm(A @ x - S), x0, bounds=[(0, None)]*256)
    x_opt = res.x

    p = x_opt[0:64]
    q = x_opt[64:128]
    r = x_opt[128:192]
    s = x_opt[192:256]
    return p,q,r,s

# Decoding p,q,r and s from the weights and bias for each model
data = np.loadtxt('public_mod.txt')   # data is 10x65 matrix where each row contains weights and bias for its respective model
for model_num, row in enumerate(data, start=1):
    p, q, r, s = my_decode(row)
    print(f"Delays for Model {model_num}:")
    print("p:", p)
    print("q:", q)
    print("r:", r)
    print("s:", s)

